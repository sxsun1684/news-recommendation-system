import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from crawler.config import NEWS_SOURCES, HEADERS
from db.news_categories import table

CACHE_EXPIRY_HOURS = 24  # è®¾ç½®æ•°æ®åº“æ•°æ®è¿‡æœŸæ—¶é—´


def save_categories_to_dynamodb(categories):
    """
    Saves the fetched news categories to AWS DynamoDB.

    - âœ… æ·»åŠ  `last_updated` å­—æ®µï¼Œè¡¨ç¤ºæ•°æ®çˆ¬å–æ—¶é—´
    """
    with table.batch_writer() as batch:
        for category in categories:
            batch.put_item(Item={
                'category_name': category[0],  # âœ… ä½œä¸ºä¸»é”®
                'category_url': category[1],
                'last_updated': datetime.utcnow().isoformat()  # â³ è®°å½•æ›´æ–°æ—¶é—´
            })
    print("âœ… Category data has been saved to DynamoDB")


def get_categories_from_dynamodb():
    """
    Retrieves category data from DynamoDB.

    - âœ… ä»…æŸ¥è¯¢ `category_name` å’Œ `category_url`
    - âœ… æŒ‰ `last_updated` æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ•°æ®
    """
    try:
        response = table.scan(ProjectionExpression="category_name, category_url, last_updated")
        items = response.get("Items", [])
        return sorted(items, key=lambda x: x["last_updated"], reverse=True)  # æŒ‰æ›´æ–°æ—¶é—´æ’åº
    except Exception as e:
        print(f"âŒ Error fetching categories from DynamoDB: {e}")
        return None


def fetch_category_links(force_update=False):
    """
    Fetches news category links from the BBC homepage navigation bar.

    - **ä¼˜å…ˆä»æ•°æ®åº“è¯»å–æ•°æ®**
    - **å¦‚æœ `force_update=True` æˆ–æ•°æ®è¿‡æœŸï¼Œé‡æ–°çˆ¬å–å¹¶æ›´æ–°æ•°æ®åº“**

    Args:
        force_update (bool): æ˜¯å¦å¼ºåˆ¶æ›´æ–°åˆ†ç±»æ•°æ®ã€‚

    Returns:
        list: æ–°é—»åˆ†ç±»æ•°æ®
    """
    # âœ… **1. å…ˆæ£€æŸ¥æ•°æ®åº“ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰å¯ç”¨æ•°æ®**
    categories = get_categories_from_dynamodb()
    if categories and not force_update:
        last_updated = datetime.fromisoformat(categories[0]["last_updated"])
        if datetime.utcnow() - last_updated < timedelta(hours=CACHE_EXPIRY_HOURS):
            print("â³ æ•°æ®æœªè¿‡æœŸï¼Œä½¿ç”¨æ•°æ®åº“ç¼“å­˜")
            return [[c["category_name"], c["category_url"]] for c in categories]

    # âœ… **2. æ•°æ®è¿‡æœŸ or `force_update=True`ï¼Œæ‰§è¡Œçˆ¬è™«**
    print("ğŸ”„ æ­£åœ¨çˆ¬å–åˆ†ç±»æ•°æ®...")
    response = requests.get(NEWS_SOURCES[0], headers=HEADERS)

    if response.status_code != 200:
        return {"error": "âŒ æ— æ³•è®¿é—® BBC ä¸»é¡µ"}

    soup = BeautifulSoup(response.text, 'html.parser')

    # å®šä½å¯¼èˆªæ 
    nav = soup.find('nav')
    if not nav:
        print("âŒ æ‰¾ä¸åˆ°å¯¼èˆªæ ")
        return []

    links = nav.find_all('a')

    categories = []
    for link in links:
        category_name = link.get_text(strip=True)
        category_url = link['href']

        # âŒ è¿‡æ»¤æ‰ç›´æ’­ã€è§†é¢‘ç­‰ä¸éœ€è¦çš„åˆ†ç±»
        if ('live' in category_url.lower() or 'video' in category_name.lower()):
            continue

        # ğŸ”— è¡¥å…¨ URL
        if category_url.startswith('/'):
            category_url = 'https://www.bbc.com' + category_url

        categories.append([category_name, category_url])

    # âœ… **3. çˆ¬å–å®Œæˆåï¼Œå­˜å…¥æ•°æ®åº“**
    save_categories_to_dynamodb(categories)
    return categories


if __name__ == "__main__":
    print(fetch_category_links())  # ğŸ›  è¿è¡Œæµ‹è¯•
